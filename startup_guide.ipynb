{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "startup_guide.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/drive/1BEiwTXZLUGwuvJHuIewckuCGPzwSLYVA?usp=sharing" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yaU3LVqXB4zm"
      },
      "source": [
        "# Load \n",
        "Load our project and install some packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZmf2eOUTDd6"
      },
      "source": [
        "! git clone https://github.com/IEEE-BITS-Pilani-Student-Chapter/robo-soccer.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TDOX9hlUetB"
      },
      "source": [
        "%cd /content/robo-soccer/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mrxUMw4cUmcl"
      },
      "source": [
        "! pip install -e ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmCFeml6U1EZ"
      },
      "source": [
        "%tensorflow_version 1.14\n",
        "! pip install keras tensorflow gym numpy stable-baselines[mpi]==2.10.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oa1O7-LD1YKq"
      },
      "source": [
        "# import packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCsZCgmjibtD"
      },
      "source": [
        "import gym\n",
        "import robo_soccer, random\n",
        "import numpy as np\n",
        "\n",
        "from IPython.display import clear_output\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from IPython import display\n",
        "\n",
        "import pymunk\n",
        "from pymunk.vec2d import Vec2d\n",
        "import pymunk.matplotlib_util"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1ujXmXmIRUH"
      },
      "source": [
        "def colab_render(env):\n",
        "      ob = env.reset()\n",
        "      done = False\n",
        "      total_reward = 0\n",
        "      while not done:\n",
        "          action = np.reshape(env.random_action(), -1)\n",
        "          ob, reward, done, info = env.step(action)\n",
        "\n",
        "          plt.clf()\n",
        "          title_str = (\"reward : \" + str(reward))\n",
        "          padding = 5\n",
        "          ax = plt.axes(xlim=(0 - padding, env.width + padding), ylim=(0 - padding, env.height + padding))\n",
        "          ax.set_aspect(\"equal\")\n",
        "          o = pymunk.matplotlib_util.DrawOptions(ax)\n",
        "          env.space.debug_draw(o)\n",
        "          plt.title(title_str, loc = 'left')\n",
        "          display.display(plt.gcf())\n",
        "          display.clear_output(wait=True)\n",
        "\n",
        "          total_reward += reward\n",
        "      return total_reward"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yhAN3yD2gZQ"
      },
      "source": [
        "# Test Envs\n",
        "Both side take random actions "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOyNIYgyIvbf"
      },
      "source": [
        "## 2 player vs 2 player"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDc-2-eHB8HJ"
      },
      "source": [
        "env = gym.make(\"Robosoccer2v2-v1\")\n",
        "env.render()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSx7TxK73_Wd"
      },
      "source": [
        "# Both side take random actions \n",
        "total_reward = colab_render(env)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K22-93tXI4j2"
      },
      "source": [
        "## 5 player vs 5 player"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bx4RJ5xgI4j4"
      },
      "source": [
        "env = gym.make(\"Robosoccer5v5-v1\")\n",
        "env.render()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glONCTOzI4j7"
      },
      "source": [
        "# Both side take random actions \n",
        "total_reward = colab_render(env)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwObqg0lJCB5"
      },
      "source": [
        "## 10 player vs 10 player"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjM97QlDJCB6"
      },
      "source": [
        "env = gym.make(\"Robosoccer-v1\")\n",
        "env.render()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mY3q-J8wJCB-"
      },
      "source": [
        "# Both side take random actions \n",
        "total_reward = colab_render(env)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JO-FcVd11eeC"
      },
      "source": [
        "# Load trained model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqNUET7jDCQj"
      },
      "source": [
        "## import stable baselines"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tx42wD6r2lQ8"
      },
      "source": [
        "import os, time, gym\n",
        "from stable_baselines.common.policies import MlpPolicy, FeedForwardPolicy, register_policy\n",
        "from stable_baselines.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
        "from stable_baselines.common import make_vec_env\n",
        "from stable_baselines import PPO2\n",
        "from stable_baselines.bench import Monitor\n",
        "from stable_baselines.common.env_checker import check_env\n",
        "from stable_baselines.common.callbacks import EvalCallback\n",
        "from stable_baselines.common.evaluation import evaluate_policy\n",
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dq11GB_lLaC9"
      },
      "source": [
        "def test_model(env, model, vis=False):\n",
        "      ob = env.reset()\n",
        "      done = False\n",
        "      total_reward = 0\n",
        "      while not done:\n",
        "          action, _states = model.predict(ob)\n",
        "          ob, reward, done, info = env.step(action)\n",
        "          if vis:\n",
        "                plt.clf()\n",
        "                title_str = \"reward : \" + str(reward)\n",
        "                padding = 5\n",
        "                ax = plt.axes(xlim=(0 - padding, env.width + padding), ylim=(0 - padding, env.height + padding))\n",
        "                ax.set_aspect(\"equal\")\n",
        "                o = pymunk.matplotlib_util.DrawOptions(ax)\n",
        "                env.space.debug_draw(o)\n",
        "                plt.title(title_str, loc = 'left')\n",
        "                display.display(plt.gcf())\n",
        "                display.clear_output(wait=True)\n",
        "          total_reward += reward\n",
        "      return total_reward"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2Mz3ywqKV16"
      },
      "source": [
        "## load \n",
        "Once you train the model and store the policy as a zip file, run and see the agent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwMYg7spKVCE"
      },
      "source": [
        "%cd /content/robo-ssoccer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCLP2JRDK2bU"
      },
      "source": [
        "model = PPO2.load(\"agent\") # replace agent with your zip file name"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VvZux3PSLJws"
      },
      "source": [
        "env = gym.make(\"Robosoccer2v2-v1\") # or whichever env you used \n",
        "test_model(env, model, vis=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pg392hnEMIN8"
      },
      "source": [
        "env = gym.make(\"Robosoccer2v2-v1\")\n",
        "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10)\n",
        "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxmn_TbqDREk"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3XNiQduuNwEx"
      },
      "source": [
        "%cd /content/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7v1bJQKVAkFE"
      },
      "source": [
        "# Custom MLP policy \n",
        "class CustomPolicy(FeedForwardPolicy):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(CustomPolicy, self).__init__(*args, **kwargs,\n",
        "                                           net_arch=[256, 256, dict(pi=[128, 128],\n",
        "                                                                    vf=[128, 128])],\n",
        "                                           feature_extraction=\"mlp\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5j8EIdBDeQM"
      },
      "source": [
        "# Separate evaluation env\n",
        "eval_env = gym.make(\"Robosoccer2v2-v1\")\n",
        "# Use deterministic actions for evaluation\n",
        "eval_callback = EvalCallback(eval_env, best_model_save_path='./logs/',\n",
        "                             log_path='./logs/', eval_freq=1000, n_eval_episodes = 5,\n",
        "                             deterministic=True, render=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02WNL00F7SYw"
      },
      "source": [
        "log_dir = \"/tmp/gym/{}\".format(int(time.time()))\n",
        "os.makedirs(log_dir, exist_ok=True)\n",
        "\n",
        "num_envs = 8\n",
        "env = gym.make(\"Robosoccer2v2-v1\")\n",
        "env = Monitor(env, log_dir, allow_early_resets=True)\n",
        "env = DummyVecEnv([lambda: env] * num_envs)\n",
        "\n",
        "model = PPO2(CustomPolicy, env, verbose=1, tensorboard_log=\"/content/PPO2_Robosoccer2v2_tensorboard/\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9l2D_onXOI8M"
      },
      "source": [
        "env = gym.make(\"Robosoccer2v2-v1\")\n",
        "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10)\n",
        "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSYAIJDi5Raq"
      },
      "source": [
        "# model training\n",
        "model.learn(total_timesteps=5 * 10**4, reset_num_timesteps=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lX6_VHtgQvyT"
      },
      "source": [
        "# model training and save the best model\n",
        "model.learn(total_timesteps=5 * 10**4, reset_num_timesteps=False, callback=eval_callback)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iiDcQ83xQv3H"
      },
      "source": [
        "env = gym.make(\"Robosoccer2v2-v1\")\n",
        "test_model(env, model, vis=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVJwfkY8iBnm"
      },
      "source": [
        "env = gym.make(\"Robosoccer2v2-v1\")\n",
        "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10)\n",
        "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kAUUpuKrz13S"
      },
      "source": [
        "# visualize the saved training process\n",
        "%tensorboard --logdir /content/PPO2_Robosoccer2v2_tensorboard/"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
